# CUB Image-Captions Unimodal VAE Image model specification
import os
import json
import numpy as np
import torch
import torch.distributions as dist
import torch.nn as nn
import torch.nn.functional as F
import torch.utils.data
from utils import Constants
from .base_vae import VAE
from .encoder_decoder_blocks.cnn_cub_text import Enc, Dec

# Constants
maxSentLen = 32  # max length of any description for birds dataset
minOccur = 3
embeddingDim = 128
lenWindow = 3
fBase = 32
vocabSize = 1590


class CUB_Sentence(VAE):
    """ Unimodal VAE subclass for Text modality CUBICC experiment """

    def __init__(self, params):
        super(CUB_Sentence, self).__init__(
            prior_dist=dist.Normal if params.priorposterior == 'Normal' else dist.Laplace,      # prior
            likelihood_dist=dist.OneHotCategorical,                                             # likelihood
            post_dist=dist.Normal if params.priorposterior == 'Normal' else dist.Laplace,       # posterior
            enc=Enc(params.latent_dim_w, params.latent_dim_z, dist=params.priorposterior),      # Encoder model
            dec=Dec(params.latent_dim_z),                                                       # Decoder model
            params=params)                                                                      # Params (args passed to main)
        self._pw_params = nn.ParameterList([
            nn.Parameter(torch.zeros(1, params.latent_dim_w), requires_grad=False),
            nn.Parameter(torch.zeros(1, params.latent_dim_w), requires_grad=True)  # It is important that this log-variance vector is learnable (see paper)
        ])
        self._pw_params_std = nn.ParameterList([
            nn.Parameter(torch.zeros(1, params.latent_dim_w), requires_grad=False),
            nn.Parameter(torch.zeros(1, params.latent_dim_w), requires_grad=False)
        ])

        self.modelName = 'cubS_resnet'
        self.llik_scaling = 1.

        self.fn_2i = lambda t: t.cpu().numpy().astype(int)
        self.fn_trun = lambda s: s[:np.where(s == 2)[0][0] + 1] if 2 in s else s
        self.vocab_file = params.tmpdir + '/CUBICC/cub.vocab'

        self.maxSentLen = maxSentLen
        self.vocabSize = vocabSize

        self.i2w = self.load_vocab()
        self.params = params

    @property
    def pw_params(self):
        """

        Returns: Parameters of prior distribution for modality-specific latent code

        """
        if self.params.priorposterior == 'Normal':
            return self._pw_params[0], F.softplus(self._pw_params[1]) + Constants.eta
        else:
            return self._pw_params[0], F.softmax(self._pw_params[1], dim=-1) * self._pw_params[1].size(-1) + Constants.eta

    @property
    def pw_params_std(self):
        """

        Returns: Parameters of prior distribution for modality-specific latent code

        """
        if self.params.priorposterior == 'Normal':
            return self._pw_params_std[0], F.softplus(self._pw_params_std[1]) + Constants.eta
        else:
            return self._pw_params_std[0], F.softmax(self._pw_params_std[1], dim=-1) * self._pw_params_std[1].size(
                -1) + Constants.eta

    # def getDataLoaders(self, batch_size, shuffle=True, device="cuda"):
    #     """Get CUBICC text modality dataloaders."""
    #     kwargs = {'num_workers': 1, 'pin_memory': True} if device == "cuda" else {}
    #     tx = lambda data: torch.Tensor(data)
    #     t_data = CUBSentences(self.params.tmpdir, split='train', one_hot=True, transpose=False, transform=tx, max_sequence_length=maxSentLen)
    #     s_data = CUBSentences(self.params.tmpdir, split='test', one_hot=True, transpose=False, transform=tx, max_sequence_length=maxSentLen)
    #
    #     train_loader = DataLoader(t_data, batch_size=batch_size, shuffle=shuffle, **kwargs)
    #     test_loader = DataLoader(s_data, batch_size=batch_size, shuffle=shuffle, **kwargs)
    #
    #     return train_loader, test_loader


    def load_vocab(self):
        # call dataloader function to create vocab file
        assert os.path.exists(self.vocab_file)
        with open(self.vocab_file, 'r') as vocab_file:
            vocab = json.load(vocab_file)
        return vocab['i2w']


    
